<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Accelerated Computing</title>
    <link>https://accelerated-computing.com/</link>
    <description>Recent content on Accelerated Computing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Oct 2023 14:25:42 +0100</lastBuildDate><atom:link href="https://accelerated-computing.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on neural networks: basic concepts</title>
      <link>https://accelerated-computing.com/blog/notes-on-neural-networks-01-basic-concepts/</link>
      <pubDate>Mon, 02 Oct 2023 14:25:42 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/notes-on-neural-networks-01-basic-concepts/</guid>
      <description>Overview A neural network is (broadly speaking) a collection of transformations arranged in a sequence of layers of neurons. A neuron is made up of some weights $W$, a bias $b$, and (frequently) a non-linear function $g$. Examples of non-linear functions typically used are ReLU, tanh, softmax, and sigmoid. A neuron in layer $i$ takes as inputs the outputs of the neurons in layer $i - 1$, applies the linear transformation encoded by its weights, adds its bias, then finally outputs the value of $g$ applied to the sum.</description>
    </item>
    
    <item>
      <title>RC: Week two</title>
      <link>https://accelerated-computing.com/blog/rc-week-two/</link>
      <pubDate>Sun, 01 Oct 2023 12:17:57 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/rc-week-two/</guid>
      <description>I&amp;rsquo;m writing this the weekend after my first fortnight at the Recurse Center. It feels like I&amp;rsquo;m beginning to find a rhythm, though I&amp;rsquo;ve noticed I can quite easily fall into a head-down, tunnel vision mode of programming, and think I should be engaging in some more pair programming. Something to aim for in the coming weeks.
My time so far has been spent either trying to learn new things, working on concrete projects, and getting to know my fellow batch-mates.</description>
    </item>
    
    <item>
      <title>RC: Week one</title>
      <link>https://accelerated-computing.com/blog/rc-week-one/</link>
      <pubDate>Mon, 18 Sep 2023 10:35:23 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/rc-week-one/</guid>
      <description>Today is the first day of the Fall 2, 2023 batch at the Recurse Center (RC), of which I am a participant.
For the unfamiliar, RC is (to quote from their home page) &amp;ldquo;The retreat where curious programmers recharge and grow.&amp;rdquo; I&amp;rsquo;m attending remotely from London, UK, for twelve weeks.
How I got here I left my job at Stripe at the start of the month. I&amp;rsquo;m proud of what I accomplished there, and how I grew as an engineer, but I came to realise that the kind of work I want to do &amp;ndash; the kind that energises me and gets me out of bed in the morning &amp;ndash; and the way the company is structured are just not compatible.</description>
    </item>
    
    <item>
      <title>Hello, world (again)</title>
      <link>https://accelerated-computing.com/blog/hello-word-again/</link>
      <pubDate>Wed, 06 Sep 2023 14:47:30 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/hello-word-again/</guid>
      <description>This is the second incarnation of this website. My first attempt took the form (loosely) of an in-progress book about GPU programming, taking inspiration somewhat from algorithmica.org. While I would one day be delighted to write such a book, this format had some drawbacks, not least of which being that I am, in fact, not (yet) a subject matter expert, and (rightly or wrongly) I felt that I had no space to write anything but fully-formed and irrefutably correct things.</description>
    </item>
    
  </channel>
</rss>