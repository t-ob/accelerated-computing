<!DOCTYPE html>
<html>
    <head><meta charset="UTF-8">

    
    <link rel="stylesheet" href="/css/styles.min.css">

<link rel="stylesheet" type="text/css" href="/css/syntax.css">


    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

</head>
    <body>
        <div class="max-w-prose prose mx-auto flex flex-col h-screen justify-between"><div>
    <h1 class="text-center">Accelerated Computing</h1>
    <div class="text-center space-x-4">
        
        <a href="/">Home</a>
        
        <a href="/blog">Blog</a>
        
        <a href="/about">About</a>
        
    </div>
</div>
<div class="mb-auto">
<h2>Notes on neural networks: positional encoding</h2>

<h3>7 December 2023</h3>

<p>
    <p>Disclaimer: this post contains content which might be just simply incorrect. I&rsquo;m still internalising some of these concepts, and my hope is the act of writing them down will help to make them a bit more concrete. This is very much a &ldquo;thinking out loud&rdquo; kind of post.</p>
<h3 id="context-windows-and-positions">Context windows and positions</h3>
<p>I will write about attention and the transformer architecture in a (/ a number of) future posts, but their study has motivated this one so I&rsquo;ll need to use a bit of that language to set the scene.</p>
<p>A transformer network takes a &ldquo;context window&rdquo; as its input. A context window is a sequence of length $n_{\text{context}}$ of (row) vectors in some embedding space of dimension $n_{\text{embed}}$. For example, if we have a two-dimensional word embedding, we might represent the sentence &ldquo;the dog is good&rdquo; as the matrix</p>
<p>$$
\begin{pmatrix}
0.1 &amp; -0.3 \\
0.6 &amp; 0.2 \\
-0.4 &amp; -0.1 \\
0.2 &amp; -0.7
\end{pmatrix}
$$</p>
<p>where eg. the embedding of &ldquo;dog&rdquo; is $(0.6, 0.2) \in \mathbb{R}^2$. In PyTorch, this might be represented by the following tensor of shape <code>(4, 2)</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">the_dog_is_good</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p>There is a point during the calculation of attention scores where information of the positions of elements within their context is lost, so that eg. (in the absence of some additional processing) the sentences &ldquo;only who can prevent forest fires&rdquo; and &ldquo;who can prevent forest fires only&rdquo; would appear as indistinguishable to the network, despite having different meanings.</p>
<h3 id="the-trick">The trick</h3>
<p>One solution to this problem is to use positional encoding. My high level intuition on this is as follows:</p>
<ol>
<li>We start with some vocabulary of size $n_{\text{vocab}}$ (eg. words in the english dictionary, <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE-generated</a> subwords of scraped internet content, etc.).</li>
<li>Each item in this vocabulary gets embedded into some $n_{\text{embed}}$ dimensional space.</li>
<li>To account for position within a given context, a new composite embedding is formed by taking the original embedding, and creating $n_{\text{context}}$ new values for each vector in its image. Each value is translated by $n_{\text{context}}$ vectors.</li>
<li>The network then takes its inputs from values this new vocabulary, which has size $n_{\text{vocab}} \cdot n_{\text{context}}$.</li>
</ol>
<p>So, continuing our example of of embeddings in space of dimension $n_{\text{embed}} = 2$, with context windows of length $n_{\text{context}} = 4$, we would like four positional translations $p_i = (p_{i, x}, p_{i, y})$ for $i = 1, \ldots, 4$, so that any $(x, y)$ occurring at position $i$ becomes $(x + p_{i, x}, y + p_{i, y})$ by the time the network sees it.</p>
<h3 id="constructing-the-embeddings">Constructing the embeddings</h3>
<p>In the paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, the authors present two concrete approaches &ndash; one with no additional network parameters and a second, simpler, version, which comes at the cost of an additional embedding matrix to learn.</p>
<h4 id="sinusoidal-method">Sinusoidal method</h4>
<p>The idea is to translate each point in the embedding space by one of $n_{\text{context}}$ points $p_i$, whose $k$-th component $p_{i, k}$ is defined to be</p>
<p>$$
p_{i, k} = \begin{cases}
\sin(\frac{i}{10000^{\frac{k}{n_{\text{embed}}}}}) &amp;\text{if } k \text{ is even,} \\
\cos(\frac{i}{10000^{\frac{k - 1}{n_{\text{embed}}}}}) &amp;\text{if } k \text{ is odd.}
\end{cases}
$$</p>
<p>Note that by considering the squares of each even-odd pair $(p_{i,k}, p_{i,k+1})$, we see that each $p_i$ lies on the sphere centered at the origin with radius $\sqrt{\frac{n_{\text{embed}}}{2}}$ &ndash; the original vocabulary is translated (by equal amounts across all possible positions) in different directions determined by each $p_i$. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>We can play around with the kinds of positional encodings for various context lengths and embedding dimensions with the following script:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">CONTEXT_LENGTH</span> <span class="o">=</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl"><span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">make_positional_embedding</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">context_length</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">coefficients</span> <span class="o">=</span> <span class="mi">10000</span> <span class="o">**</span> <span class="o">-</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">embedding_dim</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">radians</span> <span class="o">=</span> <span class="n">positions</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">coefficients</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">radians</span><span class="si">=}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">evens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">odds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">encodings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">radians</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">encodings</span><span class="p">[:,</span> <span class="n">evens</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">radians</span><span class="p">[:,</span> <span class="n">evens</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">encodings</span><span class="p">[:,</span> <span class="n">odds</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">radians</span><span class="p">[:,</span> <span class="n">odds</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">encodings</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">make_positional_embedding</span><span class="p">(</span><span class="n">CONTEXT_LENGTH</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">positional_embedding</span><span class="si">=}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">positional_embedding</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">=}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>which for the context length and embedding dims as written produces the following output:</p>
<pre tabindex="0"><code>radians=tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.0000e+00, 1.0000e+00, 1.0000e-01, 1.0000e-01, 1.0000e-02, 1.0000e-02,
         1.0000e-03, 1.0000e-03],
        [2.0000e+00, 2.0000e+00, 2.0000e-01, 2.0000e-01, 2.0000e-02, 2.0000e-02,
         2.0000e-03, 2.0000e-03],
        [3.0000e+00, 3.0000e+00, 3.0000e-01, 3.0000e-01, 3.0000e-02, 3.0000e-02,
         3.0000e-03, 3.0000e-03]])
encodings=tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,
          1.0000e+00,  0.0000e+00,  1.0000e+00],
        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,
          9.9995e-01,  1.0000e-03,  1.0000e+00],
        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,
          9.9980e-01,  2.0000e-03,  1.0000e+00],
        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,
          9.9955e-01,  3.0000e-03,  1.0000e+00]])
encodings.norm(dim=1)=tensor([2., 2., 2., 2.])
</code></pre><p>Putting this together in a PyTorch module might look like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">64</span>
</span></span><span class="line"><span class="cl"><span class="n">CONTEXT_LENGTH</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">NetWithSinusoidalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">VOCAB_SIZE</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;pos&#34;</span><span class="p">,</span> <span class="n">make_positional_embedding</span><span class="p">(</span><span class="n">CONTEXT_LENGTH</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ... other layers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ... rest of forward pass</span>
</span></span></code></pre></div><h4 id="just-let-the-network-learn-it">Just let the network learn it</h4>
<p>When I first read the above paper, this alternative approach &ndash; to just learn an embedding &ndash; seemed preferable to me, but I appreciate now the touch of class the sinusoidal approach brings to the table.</p>
<p>The idea is to equip the network with an additional embedding and let it figure out how to use it to distinguish between positions. It&rsquo;s less code, at the cost of some additional parameters to train. In PyTorch, it might look like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">64</span>
</span></span><span class="line"><span class="cl"><span class="n">CONTEXT_LENGTH</span> <span class="o">=</span> <span class="mi">8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">NetWithLearnedPositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">VOCAB_SIZE</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">CONTEXT_LENGTH</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ... other layers</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">CONTEXT_LENGTH</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ... rest of forward pass</span>
</span></span></code></pre></div><p>The paper claims that in practice, both approaches yielded more or less identical results, so both approaches appear to be about as effective as each other.</p>
<h3 id="alternate-approaches">Alternate approaches</h3>
<p>Other approaches to positional encodings exist and have been explored since the transformer architecture exploded in popularity. My fellow RC participant <a href="https://swe-to-mle.pages.dev/">Régis</a> ran a few sessions to explore these further. We looked at <a href="https://arxiv.org/abs/2104.09864">RoPE</a> and <a href="https://arxiv.org/abs/2108.12409">ALiBi</a>, both of which substitute positional encoding altogether with a modified query-key attention score process &ndash; the former by inserting rotations, and the latter by penalising scores between items further apart.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<h3 id="conclusion">Conclusion</h3>
<p>Putting these thoughts into words has made me realise there&rsquo;s a good chance I&rsquo;m missing some subtleties around positional embeddings, and indeed embeddings in general. An old colleague of mine has recommended <a href="https://sites.google.com/view/embeddings-in-nlp">Embeddings in Natural Language Processing</a> &ndash; maybe now&rsquo;s a good time to pick it up.</p>
<h3 id="further-reading">Further reading</h3>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></li>
<li><a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a></li>
<li><a href="https://sites.google.com/view/embeddings-in-nlp">Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning</a></li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I still find this surprising as the intuition I&rsquo;ve internalised, which I now believe is not correct, is that any respectable positional embedding would not perturb the original vocabulary too much, so as to preserve semantic meanings (in the case of natural language, at least). In this case as our embedding dimension grows we end up pushing our points in different positions further and further apart. Maybe semantic meaning (eg. the classic example &ldquo;king - man + woman = queen&rdquo;) has less to do with absolute position and more to do with relative position amongst neighbouring points.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Thanks also to Régis who pointed out an earlier version of this paragraph was not entirely correct!&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</p>

            </div><div class="text-center py-8">
    <script async defer src="https://www.recurse-scout.com/loader.js?t=b989d976a4bb7208d861570a04011f71"></script>
</div></div>
    </body>
</html>
