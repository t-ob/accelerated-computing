<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Accelerated Computing</title>
    <link>https://accelerated-computing.com/blog/</link>
    <description>Recent content in Blog on Accelerated Computing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Nov 2023 22:57:57 +0100</lastBuildDate><atom:link href="https://accelerated-computing.com/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on neural networks: initialisation and normalisation techniques</title>
      <link>https://accelerated-computing.com/blog/notes-on-neural-networks-part-03/</link>
      <pubDate>Fri, 10 Nov 2023 22:57:57 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/notes-on-neural-networks-part-03/</guid>
      <description>Untamed variances By construction, the gradients of one layer in a neural network are dependent on the outputs of neurons in a previous layer. A consequence of this is that one layer&amp;rsquo;s output might cause the preactivations of the next to fall within a hard-to-train regime. For example, consider the network with a single hidden layer with the following shape:
Inputs have 200 dimensions Hidden layer has both fan-in and fan-out of 1000 Outputs have 100 dimensions A naive implementation (ignoring biases, to keep things simple), which draws weights from the standard random normal distribution might look like the following:</description>
    </item>
    
    <item>
      <title>Notes on neural networks: loss, gradient descent, and backpropagation</title>
      <link>https://accelerated-computing.com/blog/notes-on-neural-networks-part-02/</link>
      <pubDate>Tue, 10 Oct 2023 22:57:57 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/notes-on-neural-networks-part-02/</guid>
      <description>Minimising loss via gradient descent When we talk about neural networks, the loss of a given network architecture on a set of example inputs is a scalar value which represents how well the model does (or doesn&amp;rsquo;t) fit those inputs1. Examples of loss functions include root mean squared error and cross-entropy.
Training a neural network involves minimising some chosen loss function $ℒ$. It is an iterative process. The most straightforward way to go about this minimisation is via the technique known as gradient descent &amp;ndash; at a high level, given some initial parameters ${\theta = (\theta_1, \ldots, \theta_k)}$ of our network2, and a loss function $ℒ$, we can implement gradient descent by repeatedly performing the following steps:</description>
    </item>
    
    <item>
      <title>Notes on neural networks: basic concepts</title>
      <link>https://accelerated-computing.com/blog/notes-on-neural-networks-01-basic-concepts/</link>
      <pubDate>Mon, 02 Oct 2023 14:25:42 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/notes-on-neural-networks-01-basic-concepts/</guid>
      <description>Overview A neural network is (broadly speaking) a collection of transformations arranged in a sequence of layers of neurons. A neuron is made up of some weights $W$, a bias $b$, and (frequently) a non-linear function $g$. Examples of non-linear functions typically used are ReLU, tanh, softmax, and sigmoid. A neuron in layer $i$ takes as inputs the outputs of the neurons in layer $i - 1$, applies the linear transformation encoded by its weights, adds its bias, then finally outputs the value of $g$ applied to the sum.</description>
    </item>
    
    <item>
      <title>RC: Week two</title>
      <link>https://accelerated-computing.com/blog/rc-week-two/</link>
      <pubDate>Sun, 01 Oct 2023 12:17:57 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/rc-week-two/</guid>
      <description>I&amp;rsquo;m writing this the weekend after my first fortnight at the Recurse Center. It feels like I&amp;rsquo;m beginning to find a rhythm, though I&amp;rsquo;ve noticed I can quite easily fall into a head-down, tunnel vision mode of programming, and think I should be engaging in some more pair programming. Something to aim for in the coming weeks.
My time so far has been spent either trying to learn new things, working on concrete projects, and getting to know my fellow batch-mates.</description>
    </item>
    
    <item>
      <title>RC: Week one</title>
      <link>https://accelerated-computing.com/blog/rc-week-one/</link>
      <pubDate>Mon, 18 Sep 2023 10:35:23 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/rc-week-one/</guid>
      <description>Today is the first day of the Fall 2, 2023 batch at the Recurse Center (RC), of which I am a participant.
For the unfamiliar, RC is (to quote from their home page) &amp;ldquo;The retreat where curious programmers recharge and grow.&amp;rdquo; I&amp;rsquo;m attending remotely from London, UK, for twelve weeks.
How I got here I left my job at Stripe at the start of the month. I&amp;rsquo;m proud of what I accomplished there, and how I grew as an engineer, but I came to realise that the kind of work I want to do &amp;ndash; the kind that energises me and gets me out of bed in the morning &amp;ndash; and the way the company is structured are just not compatible.</description>
    </item>
    
    <item>
      <title>Hello, world (again)</title>
      <link>https://accelerated-computing.com/blog/hello-word-again/</link>
      <pubDate>Wed, 06 Sep 2023 14:47:30 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/blog/hello-word-again/</guid>
      <description>This is the second incarnation of this website. My first attempt took the form (loosely) of an in-progress book about GPU programming, taking inspiration somewhat from algorithmica.org. While I would one day be delighted to write such a book, this format had some drawbacks, not least of which being that I am, in fact, not (yet) a subject matter expert, and (rightly or wrongly) I felt that I had no space to write anything but fully-formed and irrefutably correct things.</description>
    </item>
    
  </channel>
</rss>