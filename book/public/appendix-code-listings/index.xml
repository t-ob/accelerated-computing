<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Appendix: code listings on Introduction to accelerated computing</title>
    <link>https://accelerated-computing.com/appendix-code-listings/</link>
    <description>Recent content in Appendix: code listings on Introduction to accelerated computing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://accelerated-computing.com/appendix-code-listings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CUDA scalar product</title>
      <link>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-naive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-naive/</guid>
      <description>CUDA scalar product (naive) #include &amp;lt;iostream&amp;gt; #include &amp;lt;cuda.h&amp;gt; __global__ void pairwiseProducts(const float* input_x, const float* input_y, const int N, float *output) { // Get the global thread ID unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x; // Check if the thread ID is within the range of N if (idx &amp;lt; N) { // Compute the product of the corresponding elements from x and y output[idx] = input_x[idx] * input_y[idx]; } } // CUDA kernel for parallel reduction __global__ void parallelSum(const float* input, const int N, float* output) { // Define an external shared array accessible by all threads in a block extern __shared__ float sdata[]; // Get the global and local thread ID unsigned int idx = threadIdx.</description>
    </item>
    
    <item>
      <title>CUDA scalar product (cuBLAS)</title>
      <link>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-cublas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-cublas/</guid>
      <description>CUDA scalar product (cuBLAS) #include &amp;lt;iostream&amp;gt; #include &amp;lt;cublas_v2.h&amp;gt; #include &amp;lt;cuda.h&amp;gt; int main() { const int RUNS = 500; // Initialize cuBLAS cublasHandle_t handle; cublasCreate(&amp;amp;handle); const int N = 1&amp;lt;&amp;lt;28; size_t size = N * sizeof(float); float *h_x, *h_y; h_x = (float*)malloc(size); h_y = (float*)malloc(size); for (int i = 0; i &amp;lt; N; i++) { h_x[i] = 1.0; h_y[i] = 2.0; } float *d_x, *d_y; cudaMalloc(&amp;amp;d_x, size); cudaMalloc(&amp;amp;d_y, size); cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice); cudaMemcpy(d_y, h_y, size, cudaMemcpyHostToDevice); float result; for(int i = 0; i &amp;lt; RUNS; ++i) { cublasSdot(handle, N, d_x, 1, d_y, 1, &amp;amp;result); } // Clean up and return cublasDestroy(handle); cudaFree(d_x); cudaFree(d_y); free(h_x); free(h_y); return 0; } </description>
    </item>
    
    <item>
      <title>Host scalar product (naive)</title>
      <link>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-naive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-naive/</guid>
      <description>Host scalar product (naive) #include &amp;lt;cstdlib&amp;gt; float scalarProductBasic(const float* v1, const float* v2, const size_t N) { float result = 0.0; for (size_t i = 0; i &amp;lt; N; ++i) { result += v1[i] * v2[i]; } return result; } int main() { const int RUNS = 500; const int N = 1&amp;lt;&amp;lt;28; size_t size = N * sizeof(float); float *x, *y; x = (float*)malloc(size); y = (float*)malloc(size); for (int i = 0; i &amp;lt; N; i++) { x[i] = 1.</description>
    </item>
    
    <item>
      <title>Host scalar product (AVX)</title>
      <link>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-avx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-avx/</guid>
      <description>Host scalar product (AVX) #include &amp;lt;cstdlib&amp;gt; #include &amp;lt;immintrin.h&amp;gt; // AVX intrinsics float scalarProductAVX256(const float* v1, const float* v2, const size_t N) { __m256 result = _mm256_setzero_ps(); // Initialize result vector with zeros for (size_t i = 0; i &amp;lt; N; i += 8) { __m256 a = _mm256_load_ps(v1 + i); // Load 8 consecutive elements from v1 __m256 b = _mm256_load_ps(v2 + i); // Load 8 consecutive elements from v2 __m256 prod = _mm256_mul_ps(a, b); // Multiply elements result = _mm256_add_ps(result, prod); // Add to result } // Extract final result from the AVX512 register alignas(32) float output[8]; _mm256_store_ps(output, result); return output[0] + output[1] + output[2] + output[3] + output[4] + output[5] + output[6] + output[7]; } int main() { const int RUNS = 500; const int N = 1&amp;lt;&amp;lt;28; size_t size = N * sizeof(float); float *x, *y; x = (float*) std::aligned_alloc(32, size); y = (float*) std::aligned_alloc(32, size); for (int i = 0; i &amp;lt; N; i++) { x[i] = 1.</description>
    </item>
    
  </channel>
</rss>
