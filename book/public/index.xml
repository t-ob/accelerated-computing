<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction to accelerated computing</title>
    <link>https://accelerated-computing.com/</link>
    <description>Recent content on Introduction to accelerated computing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://accelerated-computing.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CUDA concepts</title>
      <link>https://accelerated-computing.com/introduction/cuda-concepts/</link>
      <pubDate>Wed, 07 Jun 2023 15:40:25 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/introduction/cuda-concepts/</guid>
      <description>CUDA concepts By the end of this chapter we will have written a simple program which does the following:
Copies data from the host machine to a GPU. Performs some computation on that data. Copies the result back to the host. Before we can do that, we need to cover some key concepts.
Kernels The CUDA programming model has at its core the concept of a kernel as its basic unit of execution.</description>
    </item>
    
    <item>
      <title>Example: scalar product</title>
      <link>https://accelerated-computing.com/introduction/example-scalar-product/</link>
      <pubDate>Wed, 07 Jun 2023 15:40:25 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/introduction/example-scalar-product/</guid>
      <description>Example: scalar product We&amp;rsquo;ll use the example of computing the scalar product of two floating point vectors. Recall that, for two vectors $x = (x_i)$ and $y = (y_i)$ of some $n$-dimensional vector space, their scalar (or dot) product $x \cdot y$ is the sum of the pairwise products of each vector&amp;rsquo;s components: $$ x \cdot y = \sum_{i=0}^{n - 1} x_i y_i $$
The scalar product is a worthwhile place to start for two reasons:</description>
    </item>
    
    <item>
      <title>Putting it all together</title>
      <link>https://accelerated-computing.com/introduction/putting-it-all-together/</link>
      <pubDate>Wed, 07 Jun 2023 15:40:25 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/introduction/putting-it-all-together/</guid>
      <description>Putting it all together At this point, we&amp;rsquo;ve written the kernels which will do our heavy lifting. We still need to call these from the host. Let&amp;rsquo;s put this all together in a complete program. It does the following:
Declares a block size of 256. Creates two constant vectors of length 1048576 and copies them to the GPU. Allocates additional memory on the GPU to store the output of their pairwise component products.</description>
    </item>
    
    <item>
      <title>Performance</title>
      <link>https://accelerated-computing.com/introduction/performance/</link>
      <pubDate>Wed, 07 Jun 2023 15:40:25 +0100</pubDate>
      
      <guid>https://accelerated-computing.com/introduction/performance/</guid>
      <description>Performance Now that we have our first CUDA program under our belt, we should ask ourselves: was it all worth it? After all, we have jumped through a fair few hoops to accomplish what can be expressed as a one-liner in Python:
def scalar_product(xs, ys): return sum(x * y for x, y in zip(xs, ys)) This is a contrived example, but it illustrates the tradeoffs involved in CUDA programming, namely performance (which we are still yet to measure!</description>
    </item>
    
    <item>
      <title>CUDA scalar product</title>
      <link>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-naive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-naive/</guid>
      <description>CUDA scalar product (naive) #include &amp;lt;iostream&amp;gt; #include &amp;lt;cuda.h&amp;gt; __global__ void pairwiseProducts(const float* input_x, const float* input_y, const int N, float *output) { // Get the global thread ID unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x; // Check if the thread ID is within the range of N if (idx &amp;lt; N) { // Compute the product of the corresponding elements from x and y output[idx] = input_x[idx] * input_y[idx]; } } // CUDA kernel for parallel reduction __global__ void parallelSum(const float* input, const int N, float* output) { // Define an external shared array accessible by all threads in a block extern __shared__ float sdata[]; // Get the global and local thread ID unsigned int idx = threadIdx.</description>
    </item>
    
    <item>
      <title>CUDA scalar product (cuBLAS)</title>
      <link>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-cublas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/cuda-scalar-product-cublas/</guid>
      <description>CUDA scalar product (cuBLAS) #include &amp;lt;iostream&amp;gt; #include &amp;lt;cublas_v2.h&amp;gt; #include &amp;lt;cuda.h&amp;gt; int main() { const int RUNS = 500; // Initialize cuBLAS cublasHandle_t handle; cublasCreate(&amp;amp;handle); const int N = 1&amp;lt;&amp;lt;28; size_t size = N * sizeof(float); float *h_x, *h_y; h_x = (float*)malloc(size); h_y = (float*)malloc(size); for (int i = 0; i &amp;lt; N; i++) { h_x[i] = 1.0; h_y[i] = 2.0; } float *d_x, *d_y; cudaMalloc(&amp;amp;d_x, size); cudaMalloc(&amp;amp;d_y, size); cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice); cudaMemcpy(d_y, h_y, size, cudaMemcpyHostToDevice); float result; for(int i = 0; i &amp;lt; RUNS; ++i) { cublasSdot(handle, N, d_x, 1, d_y, 1, &amp;amp;result); } // Clean up and return cublasDestroy(handle); cudaFree(d_x); cudaFree(d_y); free(h_x); free(h_y); return 0; } </description>
    </item>
    
    <item>
      <title>Host scalar product (naive)</title>
      <link>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-naive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-naive/</guid>
      <description>Host scalar product (naive) #include &amp;lt;cstdlib&amp;gt; float scalarProductBasic(const float* v1, const float* v2, const size_t N) { float result = 0.0; for (size_t i = 0; i &amp;lt; N; ++i) { result += v1[i] * v2[i]; } return result; } int main() { const int RUNS = 500; const int N = 1&amp;lt;&amp;lt;28; size_t size = N * sizeof(float); float *x, *y; x = (float*)malloc(size); y = (float*)malloc(size); for (int i = 0; i &amp;lt; N; i++) { x[i] = 1.</description>
    </item>
    
    <item>
      <title>Host scalar product (AVX)</title>
      <link>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-avx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://accelerated-computing.com/appendix-code-listings/host-scalar-product-avx/</guid>
      <description>Host scalar product (AVX) #include &amp;lt;cstdlib&amp;gt; #include &amp;lt;immintrin.h&amp;gt; // AVX intrinsics float scalarProductAVX256(const float* v1, const float* v2, const size_t N) { __m256 result = _mm256_setzero_ps(); // Initialize result vector with zeros for (size_t i = 0; i &amp;lt; N; i += 8) { __m256 a = _mm256_load_ps(v1 + i); // Load 8 consecutive elements from v1 __m256 b = _mm256_load_ps(v2 + i); // Load 8 consecutive elements from v2 __m256 prod = _mm256_mul_ps(a, b); // Multiply elements result = _mm256_add_ps(result, prod); // Add to result } // Extract final result from the AVX512 register alignas(32) float output[8]; _mm256_store_ps(output, result); return output[0] + output[1] + output[2] + output[3] + output[4] + output[5] + output[6] + output[7]; } int main() { const int RUNS = 500; const int N = 1&amp;lt;&amp;lt;28; size_t size = N * sizeof(float); float *x, *y; x = (float*) std::aligned_alloc(32, size); y = (float*) std::aligned_alloc(32, size); for (int i = 0; i &amp;lt; N; i++) { x[i] = 1.</description>
    </item>
    
  </channel>
</rss>
