<!DOCTYPE html>
<html lang="en"><title>Introduction to accelerated computing</title>

<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/syntax.css">
<body class="bg-white dark:bg-slate-800">
            <div class="container mx-auto"><div id="content">
<div class="flex flex-row">
    
    <div class="basis-1/4 p-4">
      <nav class="prose dark:prose-invert">
    <div>
      <a href='/'>Introduction to accelerated computing</a>
    </div>
    <ol class="list-inside [counter-reset:section]">
        
        <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.']">
            <a href="/introduction/">Introduction</a>
            <ol class="list-inside [counter-reset:section]">
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/cuda-concepts/">CUDA concepts</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/example-scalar-product/">Example: scalar product</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/putting-it-all-together/">Putting it all together</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/performance/">Performance</a>
                </li>
                
            </ol>
        </li>
        
        <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.']">
            <a href="/appendix-code-listings/">Appendix: code listings</a>
            <ol class="list-inside [counter-reset:section]">
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/cuda-scalar-product-naive/">CUDA scalar product</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/cuda-scalar-product-cublas/">CUDA scalar product (cuBLAS)</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/host-scalar-product-naive/">Host scalar product (naive)</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/host-scalar-product-avx/">Host scalar product (AVX)</a>
                </li>
                
            </ol>
        </li>
        
    </ol>
  </nav>
    </div>
    
    <div class="basis-3/4 p-4">
      <article class="prose dark:prose-invert prose-xl">
        <h2 id="performance">Performance</h2>
<p>Now that we have our first CUDA program under our belt, we should ask ourselves: was it all worth it? After all, we have jumped through a fair few hoops to accomplish what can be expressed as a one-liner in Python:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">scalar_product</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">))</span>
</span></span></code></pre></div><p>This is a contrived example, but it illustrates the tradeoffs involved in CUDA programming, namely performance (which we are still yet to measure!) that comes at the cost of simplicity.</p>
<p>In the remainder of this chapter we will make a first attempt at measuring GPU performance. Specifically, a first attempt to get a sense of how fast (or not) our program is. We keep things deliberately light, as this introduction has gone on long enough already, and the topic of GPU performance and optimisation deserves (at least!) a chapter of its own.</p>
<p>We will compare our scalar product program to the following:</p>
<ol>
<li>A naive host CPU-bound implementation in C++.</li>
<li>A second host CPU-bound implementation in C++ which uses AVX intrinsics.</li>
<li>A second CUDA program which leverages NVidia&rsquo;s cuBLAS library.</li>
</ol>
<p>We are somewhat turning a blind eye toward scientific rigour here &ndash; there are many variables at play, eg. the CPU, GPU, wider host configuration etc. &ndash; but again, if there is one place amongst these articles where some light hand waving is appropriate, it&rsquo;s here in the introduction.</p>
<p>For reference, these experiments were carried out on a system running Ubuntu 20.04, running a Ryzen 9 3900X, and an NVidia RTX 2080 Super.</p>
<p>With that disclaimer out of the way, let&rsquo;s create some benchmark programs. Each will do two things:</p>
<ol>
<li>Create two vectors of 268435456 <code>float</code>s.</li>
<li>Compute their scalar products 500 times.</li>
</ol>
<p>Why 500 times? There are a couple of reasons, but mainly I wanted to approximate a &ldquo;real&rdquo; workload (even the most naive implementation will execute once in less than a second). Furthermore, as we shall see later, moving data from host to GPU is expensive, so we amortise that cost by running our GPU calculations multiple times.</p>
<p>What follows are the relevant parts of our benchmark programs. Complete listings can be found in the <a href="/appendix-code-listings/">Appendix</a>.</p>
<h3 id="benchmark-program-cpu-naive">Benchmark program: CPU (naive)</h3>
<p>This is a straightforward implementation which reads pairs of vector elements one at a time and accumulates the result:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">scalarProductBasic</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v1</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v2</span><span class="p">,</span> <span class="k">const</span> <span class="n">size_t</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span> <span class="o">+=</span> <span class="n">v1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">v2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h3 id="benchmark-program-cpu-avx">Benchmark program: CPU (AVX)</h3>
<p>This implementation is similar to the above, but uses AVX-256<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> instructions to operate on multiple elements at a time:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;immintrin.h&gt; // AVX intrinsics</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">scalarProductAVX256</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v1</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v2</span><span class="p">,</span> <span class="k">const</span> <span class="n">size_t</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">__m256</span> <span class="n">result</span> <span class="o">=</span> <span class="n">_mm256_setzero_ps</span><span class="p">();</span> <span class="c1">// Initialize result vector with zeros
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">8</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">__m256</span> <span class="n">a</span> <span class="o">=</span> <span class="n">_mm256_load_ps</span><span class="p">(</span><span class="n">v1</span> <span class="o">+</span> <span class="n">i</span><span class="p">);</span> <span class="c1">// Load 16 consecutive elements from v1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">__m256</span> <span class="n">b</span> <span class="o">=</span> <span class="n">_mm256_load_ps</span><span class="p">(</span><span class="n">v2</span> <span class="o">+</span> <span class="n">i</span><span class="p">);</span> <span class="c1">// Load 16 consecutive elements from v2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">__m256</span> <span class="n">prod</span> <span class="o">=</span> <span class="n">_mm256_mul_ps</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span> <span class="c1">// Multiply elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">result</span> <span class="o">=</span> <span class="n">_mm256_add_ps</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">prod</span><span class="p">);</span> <span class="c1">// Add to result
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Extract final result from the AVX512 register
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">alignas</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span> <span class="kt">float</span> <span class="n">output</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">_mm256_store_ps</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">result</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">7</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h3 id="benchmark-program-gpu-cublas">Benchmark program: GPU (cuBLAS)</h3>
<p>This program is similar to our hand-rolled scalar product implementation, though uses the optimised cuBLAS library to perform its heavy lifting. We include this program as it is likely to give us an indication of a performance upper bound:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cublas_v2.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">RUNS</span> <span class="o">=</span> <span class="mi">500</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Initialize cuBLAS
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cublasHandle_t</span> <span class="n">handle</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Allocate vectors and copy to device
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">RUNS</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">cublasSdot</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Clean up and return
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h3 id="benchmark">Benchmark</h3>
<p>We have four programs: <code>scalar_product_host_naive</code>, <code>scalar_product_host_avx</code>, <code>scalar_product_cuda_naive</code> (our implementation), and <code>scalar_product_cuda_cublas</code>. Each has been compiled (we will talk about what it even means to compile a CUDA program later - another topic worthy of its own chapter) with the highest optimisation level. Let&rsquo;s see how they compare, using everyone&rsquo;s favourite command line utility <code>time</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ <span class="nb">time</span> ./scalar_product_host_naive 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">real    1m49.165s
</span></span><span class="line"><span class="cl">user    1m48.664s
</span></span><span class="line"><span class="cl">sys     0m0.492s
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ <span class="nb">time</span> ./scalar_product_host_avx 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">real    0m41.073s
</span></span><span class="line"><span class="cl">user    0m40.602s
</span></span><span class="line"><span class="cl">sys     0m0.468s
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ <span class="nb">time</span> ./scalar_product_cuda_naive 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">real    0m25.610s
</span></span><span class="line"><span class="cl">user    0m24.954s
</span></span><span class="line"><span class="cl">sys     0m0.631s
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ <span class="nb">time</span> ./scalar_product_cuda_cublas 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">real    0m4.491s
</span></span><span class="line"><span class="cl">user    0m3.670s
</span></span><span class="line"><span class="cl">sys     0m0.820s
</span></span></code></pre></div><p>The optimised cuBLAS implementation&rsquo;s runtime is less than the naive implementation on my hardware by more than a factor of 20. Even our naive implementation does well against AVX intrinsics.</p>
<p>At this stage, we shouldn&rsquo;t read too much into these measurements. However, I hope they go some way to address the question at the top of this section: was it all worth it? For me, at least, the answer is an emphatic yes.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I would have used AVX-512 instructions but the CPU available to me only supports up AVX instructions up to and including the 256-bit variants.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


        <p>





    
    

    
    

    
    

    
    

    
    

    
     
    
    

    
    

    
    

    
    

    
    

    
    




    
    

    
    

    
    

    
    

    
    

    
     
    
    

    
    

    
    

    
    

    
    

    
    



    <a href="/introduction/putting-it-all-together/">Previous</a>

 | 

    <a href="/appendix-code-listings/">Next</a>

</p>
      </article>
    </div>
  </div>

                </div></div>
    </body>
</html>
