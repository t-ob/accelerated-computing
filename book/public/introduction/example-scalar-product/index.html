<!DOCTYPE html>
<html lang="en"><title>Introduction to accelerated computing</title>

<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" type="text/css" href="/css/syntax.css">

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<body class="bg-white dark:bg-slate-800">
            <div class="container mx-auto"><div id="content">
<div class="flex flex-row">
    
    <div class="basis-1/4 p-4">
      <nav class="prose dark:prose-invert">
    <div>
      <a href='/'>Introduction to accelerated computing</a>
    </div>
    <ol class="list-inside [counter-reset:section]">
        
        <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.']">
            <a href="/introduction/">Introduction</a>
            <ol class="list-inside [counter-reset:section]">
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/cuda-concepts/">CUDA concepts</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/example-scalar-product/">Example: scalar product</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/putting-it-all-together/">Putting it all together</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/introduction/performance/">Performance</a>
                </li>
                
            </ol>
        </li>
        
        <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.']">
            <a href="/appendix-code-listings/">Appendix: code listings</a>
            <ol class="list-inside [counter-reset:section]">
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/cuda-scalar-product-naive/">CUDA scalar product</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/cuda-scalar-product-cublas/">CUDA scalar product (cuBLAS)</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/host-scalar-product-naive/">Host scalar product (naive)</a>
                </li>
                
                <li class="[counter-increment:section] marker:[content:counters(section,'.')_'.'']">
                    <a href="/appendix-code-listings/host-scalar-product-avx/">Host scalar product (AVX)</a>
                </li>
                
            </ol>
        </li>
        
    </ol>
  </nav>
    </div>
    
    <div class="basis-3/4 p-4">
      <article class="prose dark:prose-invert prose-xl">
        <h2 id="example-scalar-product">Example: scalar product</h2>
<p>We&rsquo;ll use the example of computing the scalar product of two floating point vectors. Recall that, for two vectors $x = (x_i)$ and $y = (y_i)$ of some $n$-dimensional vector space, their scalar (or dot) product $x \cdot y$ is the sum of the pairwise products of each vector&rsquo;s components:
$$
x \cdot y = \sum_{i=0}^{n - 1} x_i y_i
$$</p>
<p>The scalar product is a worthwhile place to start for two reasons:</p>
<ol>
<li>Scalar products are everywhere. Matrix multiplications are just lots of scalar products, and machine learning is just lots of matrix multiplications.</li>
<li>It is straightforward to implement, but not so trivial that we can&rsquo;t learn anything from it.</li>
</ol>
<p>We&rsquo;ll actually write two kernels &ndash; one to compute the pairwise products, and a second to compute their sum.</p>
<p>A quick caveat: what follows is likely not the most efficient way to do this &ndash; if you&rsquo;re serious about computing scalar products on a GPU, you should be using something like cuBLAS<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>!</p>
<p>The first kernel looks like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// CUDA kernel to compute the pairwise product of vector elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">pairwiseProducts</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input_x</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input_y</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">output</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Get the global thread ID
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Check if the thread ID is within the range of N
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Compute the product of the corresponding elements from a and b
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_y</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>The first line computes the global thread ID from the built-in variables described above. In this case, our data is one-dimensional, and (as we will see later), we&rsquo;ll launch the kernel with a one-dimensional configuration, so we only care about the <code>.x</code> attributes of each. Let&rsquo;s break it down a bit further:</p>
<ol>
<li><code>threadIdx.x</code>: this is the index of the executing thread within its block. There may be another thread in another block with the same index, so this is not globally unique.</li>
<li><code>blockIdx.x * blockDim.x</code>: <code>blockIdx.x</code> is the index of the block in which the thread is executing, and <code>blockDim.x</code> is the number of threads in each block. In particular, <code>blockIdx.x</code> is always greater than <code>threadIdx.x</code>, so adding a multiple of <code>blockDim.x</code> is what makes the sum result in the globally unique index.</li>
</ol>
<p>We next check if the thread has any work to do. If so, we set the output index to be the product of the inputs at the same index. As this index is unique, there are no race conditions to worry about.</p>
<p>Our second kernel is more interesting. Once we have computed our pairwise products, we now need to sum them. Recall above that we said a block of threads have access to shared memory &ndash; we&rsquo;ll make use of that feature here. We&rsquo;ll proceed as follows:</p>
<ol>
<li>Within a given block, copy a chunk (addressed by the global thread index) to some shared memory.</li>
<li>Wait for all threads to finish.</li>
<li>Consider a window over all the shared memory. Repeatedly add the right half of the window to the left half, then halve the window size and repeat until done.</li>
<li>The first element of the shared memory then contains the scalar sub-product of the thread block.</li>
</ol>
<p>We may need to call this second kernel multiple times, to reduce the results of each block to a single number.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// CUDA kernel for parallel reduction
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">parallelSum</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Define an external shared array accessible by all threads in a block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">sdata</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Get the global and local thread ID
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Load data from global to shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Sync all threads in the block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Do reduction in shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Make sure all additions at the current stage are done
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Write result of this block to global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="n">output</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">sdata</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Similar to the first kernel, we use the expression <code>threadIdx.x + blockIdx.x * blockDim.x</code> to compute a thread&rsquo;s global index. There&rsquo;s quite a bit more going on here, though. For starters, the kernel begins with the line</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">sdata</span><span class="p">[];</span>
</span></span></code></pre></div><p>This is where the block shared memory is declared. This needs a bit of unpacking &ndash; from right to left:</p>
<ol>
<li><code>float sdata[]</code>: an array of floating point numbers.</li>
<li><code>__shared__</code>: this keyword tells the CUDA compiler that the array should be placed in shared memory on the device.</li>
<li><code>extern</code>: since we do not know the size of the shared memory array at compile time, we tell the compiler it will be defined elsewhere. Specifically, in a kernel launch configuration, we can optionally provide an optional <code>size_t</code> number of bytes of shared memory to be allocated per block (see below).</li>
</ol>
<p>After thread indices established, the shared memory receives a copy of the input data addressed by the block:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// Load data from global to shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">?</span> <span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Sync all threads in the block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><p>We call <code>__syncthreads()</code> here to ensure no thread proceeds past this point until the shared memory has been written.</p>
<p>Once the shared memory is populated, the sum occurs:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// Do reduction in shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sdata</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Make sure all additions at the current stage are done
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Write result of this block to global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="n">output</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">sdata</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span></code></pre></div><p>At the start of the loop we consider a window over the entire shared memory and mark its middle as <code>s</code>. The threads whose index fall in the right hand side of this window have no more work to do at this point, while the threads in the left hand side perform the sum. At each iteration of the loop, the working thread takes its index within the block, and updates the shared memory at that index with the corresponding value in the right half of the window. We repeatedly halve the length of the window until we&rsquo;re left with the sum of the block at index zero, at which point the thread with index zero copies it back out to global memory.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://docs.nvidia.com/cuda/cublas/">https://docs.nvidia.com/cuda/cublas/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


        <p>





    
    

    
    

    
    

    
     
    
    

    
    

    
    

    
    

    
    

    
    

    
    

    
    




    
    

    
    

    
    

    
    

    
    

    
    

    
    

    
     
    
    

    
    

    
    

    
    



    <a href="/introduction/cuda-concepts/">Previous</a>

 | 

    <a href="/introduction/putting-it-all-together/">Next</a>

</p>
      </article>
    </div>
  </div>

                </div></div>
    </body>
</html>
